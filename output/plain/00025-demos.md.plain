

TRACING EXAMPLES


For most examples, we’ll assume you have two terminals side-by-side:

-   One to run the program you want to trace (referred to as tracee).
-   One to run your bpftrace and observe the output.

Note that for development cases on OS X, dtrace usage is covered
elsewhere.

The source and all of these scripts are available in the examples folder
of this repository, or from submodules.


Listing tracepoints

To list tracepoints that you can trace:

    bpftrace -l 'usdt:*' -p ${PROCESS}


Simple hello world

This is a basic ruby script that demonstrates the basic use of static
tracepoints in Ruby, using the library ruby-static-tracing
[@ruby-static-tracing] Ruby gem, covered later.

This simplistic program will loop indefinitely, printing Not Enabled
every second. This represents the Ruby program going on it’s merry way,
doing what it’s supposed to be doing - pretend that it is running actual
application code. The application isn’t spending any time executing
probes, all it is doing is checking if the probe is enabled. Since the
probe isn’t enabled, it continues with business as usual. The cost of
checking if a probe is enabled is extraordinarily low, ~5 micro
seconds).

[probetest gif]

This example:

-   Creates a provider implicitly through it’s reference to ‘global’,
    and indicates that it will be firing off an Integer and a String to
    the tracepoint.
-   Registering the tracepoint is like a function declaration - when you
    fire the tracepoint later, the fire call must match the signature
    declared by the tracepoint.
-   We fetch the the provider that we created, and enable it.
-   Enabling the provider loads it into memory, but the tracepoint isn’t
    enabled until it’s attached to.

Then, in an infinite loop, we check to see if our tracepoint is enabled,
and fire it if it is.

When we run helloworld.rb, it will loop and print:

    Not enabled
    Not enabled
    Not enabled

One line about every second. Not very interesting, right?

When we run our tracing program though:

    bpftrace -e 'usdt::global:hello_nsec
             { printf("%lld %s\n", arg0, str(arg1))}' -p $(pgrep -f helloworld.rb)

Or, using a script:

    bpftrace ./helloworld.bt -p $(pgrep -f helloworld.rb)

helloworld.bt:

We’ll notice that the output changes to indicate that the probe has been
fired:

    Not enabled
    Probe fired!
    Probe fired!
    Probe fired!

And, from our bpftrace process we see:

    Attaching 1 probe...
    55369896776138 Hello world
    55370897337512 Hello world
    55371897691043 Hello world

Upon interrupting our bpftrace with Control+C, the probe stops firing as
it is no longer enabled.

This demonstrates:

-   How to get data from ruby into our bpftrace using a tracepoint.
-   That probes are only enabled when they are attached to.
-   How to read integer and string arguments.
-   Basic usage of bpftrace with this gem.

In subsequent examples, none of these concepts are covered again.


Aggregation functions

While the hello world sample above is powerful for debugging, it’s
basically just a log statement.

To do something a little more interesting, we can use an aggregation
function.

bpftrace can generate linear and log2 histograms on map data. Linear
histograms show the same data that is used to construct an
ApDex[@appdex]. This type of tracing is good for problems like
understanding request latency.

For this example, we’ll use randist.rb to analyze a pseudo-random
distribution of data.

The example should fire out random integers between 0 and 100. We’ll see
how random it actually is with a linear histogram, bucketing the results
into steps of 10:

    bpftrace -e 'usdt::global:randist
                {@ = lhist(arg0, 0, 100, 10)}' -p $(pgrep -f ./randist.rb)
    Attaching 1 probe...

    @:
    [0, 10)      817142 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
    [10, 20)     815076 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ |
    [20, 30)     815205 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ |
    [30, 40)     814752 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ |
    [40, 50)     815183 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ |
    [50, 60)     816676 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ |
    [60, 70)     816470 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ |
    [70, 80)     815448 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ |
    [80, 90)     816913 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ |
    [90, 100)    814970 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ |

There are similar aggregation functions [@bpftrace-reference-guide] for
max, mean, count, etc that can be used to summarize large data sets -
check them out!


Latency distributions

This example will profile the function call that we use for getting the
current monotonic time in nanoseconds:

    StaticTracing.nsec

Under the hood, this is just calling a libc function to get the current
time against a monotonic source. This is how we calculate the latency in
wall-clock time. Since we will be potentially running this quite a lot,
we want it to be fast!

For this example, we’ll use nsec.rb script to compute the latency of
this call and fire it off in a probe.

Attaching to it with a log2 histogram, we can see that it clusters
within a particular latency range:

    bpftrace -e 'usdt::global:nsec_latency
                {@ = hist(arg0)}' -p $(pgrep -f ./nsec.rb)
    Attaching 1 probe...

    @:
    [256, 512)            65 |                                                    |
    [512, 1K)            162 |@@                                                  |
    [1K, 2K)            3647 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
    [2K, 4K)            3250 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@      |
    [4K, 8K)               6 |                                                    |
    [8K, 16K)              0 |                                                    |
    [16K, 32K)            12 |                                                    |
    [32K, 64K)             2 |                                                    |

Let’s zoom in on that with a linear histogram to get a better idea of
the latency distribution:

    bpftrace -e 'usdt::global:nsec_latency
                {@ = lhist(arg0, 0, 3000, 100)}' -p $(pgrep -f ./nsec.rb)
    Attaching 1 probe...

    @:
    [300, 400)         1 |                                                    |
    [400, 500)        33 |@                                                   |
    [500, 600)        50 |@@                                                  |
    [600, 700)        49 |@@                                                  |
    [700, 800)        42 |@@                                                  |
    [800, 900)        21 |@                                                   |
    [900, 1000)       15 |                                                    |
    [1000, 1100)       9 |                                                    |
    [1100, 1200)      11 |                                                    |
    [1200, 1300)       4 |                                                    |
    [1300, 1400)      16 |                                                    |
    [1400, 1500)       9 |                                                    |
    [1500, 1600)       7 |                                                    |
    [1600, 1700)       8 |                                                    |
    [1700, 1800)      70 |@@@                                                 |
    [1800, 1900)     419 |@@@@@@@@@@@@@@@@@@@@@                               |
    [1900, 2000)     997 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
    [2000, 2100)     564 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                       |
    [2100, 2200)      98 |@@@@@                                               |
    [2200, 2300)      37 |@                                                   |
    [2300, 2400)      30 |@                                                   |
    [2400, 2500)      36 |@                                                   |
    [2500, 2600)      46 |@@                                                  |
    [2600, 2700)      86 |@@@@                                                |
    [2700, 2800)      74 |@@@                                                 |
    [2800, 2900)      42 |@@                                                  |
    [2900, 3000)      26 |@                                                   |
    [3000, ...)       35 |@                                                   |

We can see that most of the calls are happening within 1700-2200
nanoseconds, which is pretty blazing fast, around 1-2 microseconds. Some
are faster, and some are slower, representing the long-tails of this
distribution, but this can give us confidence that this call will
complete quickly.
